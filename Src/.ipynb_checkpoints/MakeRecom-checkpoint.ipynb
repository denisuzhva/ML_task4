{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metadata\n",
    "\n",
    "NODES = 196591\n",
    "EDGES = 950327\n",
    "FULL_EDGES = 2097245\n",
    "CHECKINS = 6442892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Check-ins and permute them\n",
    "\n",
    "ci_dataset = np.load('../Dataset/ci_ids.npy') # [usr, place]\n",
    "p = np.random.permutation(CHECKINS)\n",
    "ci_dataset = ci_dataset[p, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy with no clusterization (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 107092\n"
     ]
    }
   ],
   "source": [
    "## Count users\n",
    "\n",
    "u_usr = np.unique(ci_dataset[:, 0]).astype(np.int32)\n",
    "n_usr = u_usr.size\n",
    "print('Unique users: %i' % n_usr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train users: 85674\n",
      "Number of test users: 21418\n",
      "Number of train check-ins: 5161157\n",
      "Number of test check-ins: 1281735\n"
     ]
    }
   ],
   "source": [
    "## Divide into train and test data\n",
    "\n",
    "test_size = n_usr // 5 # test dataset is 5 times smaller than the full dataset\n",
    "print('Number of train users: %i' % (n_usr - test_size))\n",
    "print('Number of test users: %i' % test_size)\n",
    "\n",
    "# permute users\n",
    "p = np.random.permutation(n_usr)\n",
    "u_usr_rand = u_usr[p]\n",
    "\n",
    "# divide users\n",
    "test_usrs = u_usr_rand[:test_size]\n",
    "train_usrs = u_usr_rand[test_size:]\n",
    "\n",
    "# mark arguments of test and train checkins in ci_dataset\n",
    "test_checkins_args = np.argwhere(np.in1d(ci_dataset[:, 0], test_usrs)).flatten()\n",
    "train_checkins_args = np.argwhere(np.in1d(ci_dataset[:, 0], train_usrs)).flatten()\n",
    "print('Number of train check-ins: %i' % train_checkins_args.size)\n",
    "print('Number of test check-ins: %i' % test_checkins_args.size)\n",
    "\n",
    "# get checkins\n",
    "test_checkins = ci_dataset[test_checkins_args, 1].flatten()\n",
    "train_checkins = ci_dataset[train_checkins_args, 1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count checkins\n",
    "\n",
    "uniq_train, cnt_train = np.unique(train_checkins, return_counts=True)\n",
    "uniq_test, cnt_test = np.unique(test_checkins, return_counts=True)\n",
    "\n",
    "# concatenate unique elements and their counts\n",
    "train_data = np.concatenate((uniq_train.reshape(-1, 1),\n",
    "                             cnt_train.reshape(-1, 1)),\n",
    "                             axis=1).astype(np.int32)\n",
    "test_data = np.concatenate((uniq_test.reshape(-1, 1),\n",
    "                            cnt_test.reshape(-1, 1)),\n",
    "                            axis=1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Ratings\n",
      "    Place ID  Count\n",
      "1      55033   4822\n",
      "2      19542   4617\n",
      "3       9410   3733\n",
      "4      10259   3345\n",
      "5      14470   2809\n",
      "6      58725   2787\n",
      "7       9246   2763\n",
      "8      23256   2721\n",
      "9      10190   2718\n",
      "10      9241   2550\n"
     ]
    }
   ],
   "source": [
    "## Sort with respect to counts and then flip to make ratings\n",
    "\n",
    "train_data_srt = train_data[train_data[:, 1].argsort()]\n",
    "train_data_srt = np.flip(train_data_srt, axis=0)\n",
    "\n",
    "# visualization\n",
    "train_dataframe = pd.DataFrame(data=train_data_srt[:10],\n",
    "                               index=np.arange(1, 11),\n",
    "                               columns=['Place ID', 'Count'])\n",
    "print('Train Ratings')\n",
    "print(train_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.037\n"
     ]
    }
   ],
   "source": [
    "## Calculate the accuracy metric: hits / (k * test_size) where k = 10\n",
    "\n",
    "# get top 10 locations from train data\n",
    "train_top10_loc = train_data_srt[:10, 0]\n",
    "\n",
    "# find counts in the test dataset that correspond to those locations\n",
    "coinc = np.argwhere(np.in1d(test_data[:, 0].flatten(), train_top10_loc)).flatten()\n",
    "sum_test_counts = np.sum(test_data[coinc][:, 1])\n",
    "acc = sum_test_counts / (10 * test_size)\n",
    "print('Baseline accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy with clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11483,)\n"
     ]
    }
   ],
   "source": [
    "## Load array of exemplars\n",
    "\n",
    "exemp = np.load('./exemplars.npy')\n",
    "exemp_unique = np.unique(exemp) # unique exemplars\n",
    "print(exemp_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96542,)\n"
     ]
    }
   ],
   "source": [
    "## Make a dictionary of clusters\n",
    "\n",
    "clust_dict = dict()\n",
    "for exemplar in exemp_unique:\n",
    "    clust_dict[exemplar] = np.argwhere(exemp == exemplar).flatten()\n",
    "\n",
    "print(clust_dict[exemp_unique[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make top 10 for each cluster (among the users in the train selection)\n",
    "\n",
    "clust_top10 = dict()\n",
    "for exemplar in exemp_unique:\n",
    "    train_clust_args = np.argwhere(np.in1d(train_usrs, clust_dict[exemplar]))\n",
    "    train_clust_checkins = ci_dataset[train_clust_args, 1].flatten()\n",
    "    uniq_clust_train, cnt_clust_train = np.unique(train_clust_checkins, return_counts=True)\n",
    "    train_clust_data = np.concatenate((uniq_clust_train.reshape(-1, 1),\n",
    "                                       cnt_clust_train.reshape(-1, 1)),\n",
    "                                       axis=1).astype(np.int32)\n",
    "    train_clust_data_srt = train_clust_data[train_clust_data[:, 1].argsort()]\n",
    "    train_clust_data_srt = np.flip(train_clust_data_srt, axis=0)\n",
    "    clust_top10[exemplar] = train_clust_data_srt[:10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2842666ec774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mexemplar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexemp_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest_clust_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclust_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexemplar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "## Calculate the accuracy metric for the clusters\n",
    "\n",
    "#for exemplar in exemp_unique:\n",
    "#    test_clust_args = np.argwhere(np.in1d(test, clust_dict[exemplar]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
